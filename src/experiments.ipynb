{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeLimit\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menv_hiv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HIVPatient\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHIVPatient\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"HIV patient simulator\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Implements the simulator defined in 'Dynamic Multidrug Therapies for HIV: Optimal and STI Control Approaches' by Adams et al. (2004).\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    The transition() function allows to simulate continuous time dynamics and control.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    The step() function is tailored for the evaluation of Structured Treatment Interruptions.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mself\u001b[39m, clipping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, logscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, domain_randomization: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     ):\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:137\u001b[0m, in \u001b[0;36mHIVPatient\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m  \u001b[38;5;66;03m# saturation constant for death (cells per mL)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeltaE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# natural death rate (per day)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, seed: \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munhealthy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m ):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muninfected\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e6\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "from env_hiv import HIVPatient\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "env = TimeLimit(\n",
    "    env=HIVPatient(domain_randomization=False), max_episode_steps=200\n",
    ")  # The time wrapper limits the number of steps in an episode at 200.\n",
    "# Now is the floor is yours to implement the agent and train it.\n",
    "\n",
    "\n",
    "# You have to implement your own agent.\n",
    "# Don't modify the methods names and signatures, but you can add methods.\n",
    "# ENJOY!\n",
    "class ProjectAgent:\n",
    "    def act(self, observation, use_random=False):\n",
    "        return 0\n",
    "\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "def rf_fqi(S, A, R, S2, D, iterations, nb_actions, gamma, disable_tqdm=False):\n",
    "    nb_samples = S.shape[0]\n",
    "    Qfunctions = []\n",
    "    SA = np.append(S,A,axis=1)\n",
    "    for iter in tqdm(range(iterations), disable=disable_tqdm):\n",
    "        if iter==0:\n",
    "            value=R.copy()\n",
    "        else:\n",
    "            Q2 = np.zeros((nb_samples,nb_actions))\n",
    "            for a2 in range(nb_actions):\n",
    "                A2 = a2*np.ones((S.shape[0],1))\n",
    "                S2A2 = np.append(S2,A2,axis=1)\n",
    "                Q2[:,a2] = Qfunctions[-1].predict(S2A2)\n",
    "            max_Q2 = np.max(Q2,axis=1)\n",
    "            value = R + gamma*(1-D)*max_Q2\n",
    "        Q = RandomForestRegressor()\n",
    "        Q.fit(SA,value)\n",
    "        Qfunctions.append(Q)\n",
    "    return Qfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n \n",
    "\n",
    "\n",
    "# %load solutions/replay_buffer2.py\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "        self.device = device\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n",
    "        return torch.argmax(Q).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class dqn_agent:\n",
    "    def __init__(self, config, model):\n",
    "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.gamma = config['gamma']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.memory = ReplayBuffer(config['buffer_size'], device)\n",
    "        self.epsilon_max = config['epsilon_max']\n",
    "        self.epsilon_min = config['epsilon_min']\n",
    "        self.epsilon_stop = config['epsilon_decay_period']\n",
    "        self.epsilon_delay = config['epsilon_delay_decay']\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = model \n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            QYmax = self.model(Y).max(1)[0].detach()\n",
    "            #update = torch.addcmul(R, self.gamma, 1-D, QYmax)\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "\n",
    "            # select epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(self.model, state)\n",
    "\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.memory.append(state, action, reward, next_state, done)\n",
    "            episode_cum_reward += reward\n",
    "\n",
    "            # train\n",
    "            self.gradient_step()\n",
    "\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done:\n",
    "                episode += 1\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", batch size \", '{:5d}'.format(len(self.memory)), \n",
    "                      \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
    "                      sep='')\n",
    "                state, _ = env.reset()\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m     26\u001b[0m agent \u001b[38;5;241m=\u001b[39m dqn_agent(config, DQN)\n\u001b[0;32m---> 27\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(scores)\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mdqn_agent.train\u001b[0;34m(self, env, max_episode)\u001b[0m\n\u001b[1;32m     51\u001b[0m     action \u001b[38;5;241m=\u001b[39m greedy_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, state)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# step\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m next_state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend(state, action, reward, next_state, done)\n\u001b[1;32m     56\u001b[0m episode_cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/monenv_python310/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:231\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[0;34m(self, a_index)\u001b[0m\n\u001b[1;32m    229\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[1;32m    230\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[0;32m--> 231\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:212\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[0;34m(self, state, action, duration)\u001b[0m\n\u001b[1;32m    210\u001b[0m nb_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[0;32m--> 212\u001b[0m     der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# np.clip(state1, self.lower, self.upper, out=state1)\u001b[39;00m\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:204\u001b[0m, in \u001b[0;36mHIVPatient.der\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    189\u001b[0m Vdot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNT \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps2) \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m*\u001b[39m V\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;241m*\u001b[39m V\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    198\u001b[0m Edot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdaE\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbE \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star) \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m/\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKb)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdE \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star) \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m/\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKd)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeltaE \u001b[38;5;241m*\u001b[39m E\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mT1dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT1stardot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2stardot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVdot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEdot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Declare network\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n \n",
    "nb_neurons=24\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "# DQN config\n",
    "config = {'nb_actions': env.action_space.n,\n",
    "          'learning_rate': 0.001,\n",
    "          'gamma': 0.95,\n",
    "          'buffer_size': 20,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 20}\n",
    "\n",
    "# Train agent\n",
    "agent = dqn_agent(config, DQN)\n",
    "scores = agent.train(env, 200)\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Episode   1, epsilon   1.00, batch size   200, episode return 1.37e+07, validation score 0.00e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialization of the agent. Replace DummyAgent with your custom agent implementation.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m agent \u001b[38;5;241m=\u001b[39m ProjectAgent()\n\u001b[0;32m---> 23\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m agent\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Keep the following lines to evaluate your agent unchanged.\u001b[39;00m\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/train.py:211\u001b[0m, in \u001b[0;36mProjectAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, state)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# step\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m next_state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend(state, action, reward, next_state, done)\n\u001b[1;32m    213\u001b[0m episode_cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/monenv_python310/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:231\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[0;34m(self, a_index)\u001b[0m\n\u001b[1;32m    229\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[1;32m    230\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[0;32m--> 231\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:212\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[0;34m(self, state, action, duration)\u001b[0m\n\u001b[1;32m    210\u001b[0m nb_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[0;32m--> 212\u001b[0m     der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# np.clip(state1, self.lower, self.upper, out=state1)\u001b[39;00m\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:204\u001b[0m, in \u001b[0;36mHIVPatient.der\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    189\u001b[0m Vdot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNT \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps2) \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m*\u001b[39m V\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;241m*\u001b[39m V\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    198\u001b[0m Edot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdaE\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbE \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star) \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m/\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKb)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdE \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star) \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m/\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKd)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeltaE \u001b[38;5;241m*\u001b[39m E\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mT1dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT1stardot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2stardot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVdot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEdot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population\n",
    "from train import ProjectAgent  # Replace DummyAgent with your agent implementation\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(seed=42)\n",
    "# Initialization of the agent. Replace DummyAgent with your custom agent implementation.\n",
    "agent = ProjectAgent()\n",
    "agent.train()\n",
    "agent.load()\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Episode   1, epsilon   1.00, batch size   200, episode return 6.23e+06, validation score 0.00e+00\n",
      "Episode   2, epsilon   0.99, batch size   400, episode return 7.21e+06, validation score 0.00e+00\n",
      "Episode   3, epsilon   0.98, batch size   600, episode return 4.62e+06, validation score 0.00e+00\n",
      "Episode   4, epsilon   0.97, batch size   800, episode return 5.33e+06, validation score 0.00e+00\n",
      "Episode   5, epsilon   0.96, batch size  1000, episode return 1.34e+07, validation score 0.00e+00\n",
      "Episode   6, epsilon   0.95, batch size  1200, episode return 8.87e+06, validation score 0.00e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Initialization of the agent. Replace DummyAgent with your custom agent implementation.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m ProjectAgent()\n\u001b[0;32m----> 4\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m agent\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Keep the following lines to evaluate your agent unchanged.\u001b[39;00m\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/train.py:211\u001b[0m, in \u001b[0;36mProjectAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, state)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# step\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m next_state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend(state, action, reward, next_state, done)\n\u001b[1;32m    213\u001b[0m episode_cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/monenv_python310/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:231\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[0;34m(self, a_index)\u001b[0m\n\u001b[1;32m    229\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[1;32m    230\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[0;32m--> 231\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:212\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[0;34m(self, state, action, duration)\u001b[0m\n\u001b[1;32m    210\u001b[0m nb_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[0;32m--> 212\u001b[0m     der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# np.clip(state1, self.lower, self.upper, out=state1)\u001b[39;00m\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:204\u001b[0m, in \u001b[0;36mHIVPatient.der\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    189\u001b[0m Vdot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNT \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps2) \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m*\u001b[39m V\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;241m*\u001b[39m V\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    198\u001b[0m Edot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdaE\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbE \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star) \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m/\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKb)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdE \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star) \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m/\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKd)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeltaE \u001b[38;5;241m*\u001b[39m E\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mT1dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT1stardot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2stardot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVdot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEdot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed_everything(seed=42)\n",
    "# Initialization of the agent. Replace DummyAgent with your custom agent implementation.\n",
    "agent = ProjectAgent()\n",
    "agent.train()\n",
    "agent.load()\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from functools import partial\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "from env_hiv import HIVPatient\n",
    "from interface import Agent\n",
    "\n",
    "\n",
    "def evaluate_agent(agent: Agent, env: gym.Env, nb_episode: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an agent in a given environment.\n",
    "\n",
    "    Args:\n",
    "        agent (Agent): The agent to evaluate.\n",
    "        env (gym.Env): The environment to evaluate the agent in.\n",
    "        nb_episode (int): The number of episode to evaluate the agent.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean reward of the agent over the episodes.\n",
    "    \"\"\"\n",
    "    rewards: list[float] = []\n",
    "    for _ in range(nb_episode):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            action = agent.act(obs)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return mean(rewards)\n",
    "\n",
    "\n",
    "evaluate_HIV = partial(\n",
    "    evaluate_agent, env=TimeLimit(HIVPatient(), max_episode_steps=200)\n",
    ")\n",
    "\n",
    "\n",
    "evaluate_HIV_population = partial(\n",
    "    evaluate_agent,\n",
    "    env=TimeLimit(HIVPatient(domain_randomization=True), max_episode_steps=200),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m agent\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Keep the following lines to evaluate your agent unchanged.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m score_agent: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_HIV\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m score_agent_dr: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m evaluate_HIV_population(agent\u001b[38;5;241m=\u001b[39magent, nb_episode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/evaluate.py:30\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(agent, env, nb_episode)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated:\n\u001b[1;32m     29\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[0;32m---> 30\u001b[0m     obs, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     32\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n",
      "File \u001b[0;32m~/monenv_python310/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:231\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[0;34m(self, a_index)\u001b[0m\n\u001b[1;32m    229\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[1;32m    230\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[0;32m--> 231\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:212\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[0;34m(self, state, action, duration)\u001b[0m\n\u001b[1;32m    210\u001b[0m nb_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[0;32m--> 212\u001b[0m     der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# np.clip(state1, self.lower, self.upper, out=state1)\u001b[39;00m\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/env_hiv.py:204\u001b[0m, in \u001b[0;36mHIVPatient.der\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    189\u001b[0m Vdot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNT \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps2) \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m*\u001b[39m V\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;241m*\u001b[39m V\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    198\u001b[0m Edot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdaE\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbE \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star) \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m/\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKb)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdE \u001b[38;5;241m*\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star) \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m/\u001b[39m (T1star \u001b[38;5;241m+\u001b[39m T2star \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKd)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeltaE \u001b[38;5;241m*\u001b[39m E\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mT1dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT1stardot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2dot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT2stardot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVdot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEdot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population\n",
    "from train import ProjectAgent  # Replace DummyAgent with your agent implementation\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "\n",
    "seed_everything(seed=42)\n",
    "# Initialization of the agent. Replace DummyAgent with your custom agent implementation.\n",
    "agent = ProjectAgent()\n",
    "agent.load()\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "from env_hiv import HIVPatient\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population\n",
    "\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "env = TimeLimit(\n",
    "    env=HIVPatient(domain_randomization=True), max_episode_steps=200\n",
    ")  # The time wrapper limits the number of steps in an episode at 200.\n",
    "# Now is the floor is yours to implement the agent and train it.\n",
    "\n",
    "\n",
    "# You have to implement your own agent.\n",
    "# Don't modify the methods names and signatures, but you can add methods.\n",
    "# ENJOY!\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "        self.device = device\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "class ProjectAgent:\n",
    " \n",
    "\n",
    "    def act(self, observation, use_random=False):\n",
    "\n",
    "        device = \"cuda\" if next(self.model.parameters()).is_cuda else \"cpu\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q = self.model(torch.Tensor(observation).unsqueeze(0).to(device))\n",
    "            return torch.argmax(Q).item()\n",
    "\n",
    "    def load(self):\n",
    "        device = torch.device('cpu')\n",
    "        self.path = os.getcwd() + \"/model4.pt\"\n",
    "        self.model = self.network({}, device)\n",
    "        self.model.load_state_dict(torch.load(self.path, map_location=device))\n",
    "        self.model.eval()\n",
    "        return \n",
    "    \n",
    "    def save(self, path):\n",
    "        self.path = path + \"/model4.pt\"\n",
    "        torch.save(self.model.state_dict(), self.path)\n",
    "        return \n",
    "\n",
    "    \n",
    "    def dqn_agent(self, config, device):\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        n_action = env.action_space.n \n",
    "        nb_neurons=256 \n",
    "        DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, nb_neurons), \n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "        return DQN\n",
    "\n",
    "    \n",
    "    def greedy_action(self, network, state):\n",
    "        device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "        with torch.no_grad():\n",
    "            Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n",
    "            return torch.argmax(Q).item()\n",
    "\n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "            \n",
    "    def train(self):\n",
    "\n",
    "        ## CONFIGURE NETWORK\n",
    "        # DQN config (change here for better results?)\n",
    "        config = {'nb_actions': env.action_space.n,\n",
    "                'learning_rate': 0.001,\n",
    "                'gamma': 0.98,\n",
    "                'buffer_size': 100000,\n",
    "                'epsilon_min': 0.02,\n",
    "                'epsilon_max': 1.,\n",
    "                'epsilon_decay_period': 20000, # go plus haut? plus bas ?\n",
    "                'epsilon_delay_decay': 100,\n",
    "                'batch_size': 800,\n",
    "                'gradient_steps': 3,\n",
    "                'update_target_strategy': 'replace', # or 'ema'\n",
    "                'update_target_freq': 400,\n",
    "                'update_target_tau': 0.005,\n",
    "                'criterion': torch.nn.SmoothL1Loss()}\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available()  else \"cpu\" #\n",
    "        self.nb_actions = config['nb_actions'] #\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95 #\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100 #\n",
    "        self.model= self.dqn_agent(config, device) ######\n",
    "        self.target_model = deepcopy(self.model).to(device)####\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory = ReplayBuffer(buffer_size, device) #\n",
    "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1. #\n",
    "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01 #\n",
    "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000 #\n",
    "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20 #\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop #\n",
    "        \n",
    "        \n",
    "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss() #\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001 #\n",
    "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr) #\n",
    "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1 #\n",
    "        self.update_target_strategy = config['update_target_strategy'] if 'update_target_strategy' in config.keys() else 'replace' #\n",
    "        self.update_target_freq = config['update_target_freq'] if 'update_target_freq' in config.keys() else 20 #\n",
    "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005 #\n",
    "\n",
    "        max_episode=256\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        actual_val_score_pop=0\n",
    "        actual_val_score_ind=0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "            # select epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = self.greedy_action(self.model, state)\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.memory.append(state, action, reward, next_state, done)\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_model.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                self.target_model.load_state_dict(target_state_dict)\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done or trunc:\n",
    "                episode += 1\n",
    "                new_val_score_ind=evaluate_HIV(agent=agent, nb_episode=1)\n",
    "                new_val_score_pop= evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", batch size \", '{:5d}'.format(len(self.memory)), \n",
    "                      \", episode return \", '{:.1e}'.format(episode_cum_reward),\n",
    "                        \", actual_val_score_ind \", '{:.1e}'.format(actual_val_score_ind),\n",
    "                        \", actual_val_score_pop \", '{:.1e}'.format(actual_val_score_pop),\n",
    "                      sep='')\n",
    "                state, _ = env.reset()\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                episode_cum_reward = 0\n",
    "                if new_val_score_ind> actual_val_score_ind:\n",
    "                    actual_val_score_ind=new_val_score_ind\n",
    "                    print(\"better model\")\n",
    "                    improved_model = deepcopy(self.model).to(device)\n",
    "                    path = os.getcwd()\n",
    "                    self.save(path)\n",
    "                    \n",
    "            else:\n",
    "                state = next_state\n",
    "        self.model.load_state_dict(improved_model.state_dict())\n",
    "        path = os.getcwd()\n",
    "        self.save(path)\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Episode   1, epsilon   1.00, batch size   200, episode return 1.7e+07, actual_val_score_ind 3.2e+06\n",
      "better model\n",
      "Episode   2, epsilon   0.99, batch size   400, episode return 1.4e+07, actual_val_score_ind 3.2e+06\n",
      "Episode   3, epsilon   0.98, batch size   600, episode return 7.9e+06, actual_val_score_ind 3.2e+06\n",
      "Episode   4, epsilon   0.97, batch size   800, episode return 6.4e+06, actual_val_score_ind 3.2e+06\n",
      "Episode   5, epsilon   0.96, batch size  1000, episode return 1.0e+07, actual_val_score_ind 3.2e+06\n",
      "Episode   6, epsilon   0.95, batch size  1200, episode return 9.0e+06, actual_val_score_ind 3.7e+06\n",
      "better model\n",
      "Episode   7, epsilon   0.94, batch size  1400, episode return 6.9e+06, actual_val_score_ind 3.4e+06\n",
      "Episode   8, epsilon   0.93, batch size  1600, episode return 7.3e+06, actual_val_score_ind 3.4e+06\n",
      "Episode   9, epsilon   0.92, batch size  1800, episode return 1.2e+07, actual_val_score_ind 6.4e+06\n",
      "better model\n",
      "Episode  10, epsilon   0.91, batch size  2000, episode return 1.2e+07, actual_val_score_ind 9.3e+06\n",
      "better model\n",
      "Episode  11, epsilon   0.90, batch size  2200, episode return 1.0e+07, actual_val_score_ind 9.3e+06\n",
      "better model\n",
      "Episode  12, epsilon   0.89, batch size  2400, episode return 1.1e+07, actual_val_score_ind 6.4e+06\n",
      "Episode  13, epsilon   0.88, batch size  2600, episode return 1.0e+07, actual_val_score_ind 8.7e+06\n",
      "Episode  14, epsilon   0.87, batch size  2800, episode return 9.4e+06, actual_val_score_ind 9.7e+06\n",
      "better model\n",
      "Episode  15, epsilon   0.86, batch size  3000, episode return 1.9e+07, actual_val_score_ind 6.2e+06\n",
      "Episode  16, epsilon   0.85, batch size  3200, episode return 1.9e+07, actual_val_score_ind 8.9e+06\n",
      "Episode  17, epsilon   0.84, batch size  3400, episode return 1.1e+07, actual_val_score_ind 1.2e+07\n",
      "better model\n",
      "Episode  18, epsilon   0.83, batch size  3600, episode return 1.2e+07, actual_val_score_ind 9.3e+06\n",
      "Episode  19, epsilon   0.82, batch size  3800, episode return 7.0e+06, actual_val_score_ind 8.5e+06\n",
      "Episode  20, epsilon   0.81, batch size  4000, episode return 1.2e+07, actual_val_score_ind 9.3e+06\n",
      "Episode  21, epsilon   0.80, batch size  4200, episode return 8.5e+06, actual_val_score_ind 9.3e+06\n",
      "Episode  22, epsilon   0.79, batch size  4400, episode return 1.3e+07, actual_val_score_ind 9.3e+06\n",
      "Episode  23, epsilon   0.78, batch size  4600, episode return 8.8e+06, actual_val_score_ind 1.1e+07\n",
      "Episode  24, epsilon   0.77, batch size  4800, episode return 1.8e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  25, epsilon   0.76, batch size  5000, episode return 1.3e+07, actual_val_score_ind 9.1e+06\n",
      "Episode  26, epsilon   0.75, batch size  5200, episode return 1.8e+07, actual_val_score_ind 1.1e+07\n",
      "Episode  27, epsilon   0.74, batch size  5400, episode return 1.6e+07, actual_val_score_ind 1.1e+07\n",
      "Episode  28, epsilon   0.73, batch size  5600, episode return 1.6e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  29, epsilon   0.72, batch size  5800, episode return 2.1e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  30, epsilon   0.71, batch size  6000, episode return 1.5e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  31, epsilon   0.70, batch size  6200, episode return 1.3e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  32, epsilon   0.69, batch size  6400, episode return 2.6e+07, actual_val_score_ind 1.3e+07\n",
      "better model\n",
      "Episode  33, epsilon   0.68, batch size  6600, episode return 9.8e+06, actual_val_score_ind 1.1e+07\n",
      "Episode  34, epsilon   0.67, batch size  6800, episode return 3.8e+07, actual_val_score_ind 1.1e+07\n",
      "Episode  35, epsilon   0.66, batch size  7000, episode return 2.3e+07, actual_val_score_ind 1.3e+07\n",
      "better model\n",
      "Episode  36, epsilon   0.65, batch size  7200, episode return 1.8e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  37, epsilon   0.64, batch size  7400, episode return 1.2e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  38, epsilon   0.63, batch size  7600, episode return 1.5e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  39, epsilon   0.62, batch size  7800, episode return 2.9e+07, actual_val_score_ind 1.3e+07\n",
      "Episode  40, epsilon   0.61, batch size  8000, episode return 3.0e+07, actual_val_score_ind 2.1e+07\n",
      "better model\n",
      "Episode  41, epsilon   0.60, batch size  8200, episode return 2.4e+07, actual_val_score_ind 1.3e+07\n",
      "Episode  42, epsilon   0.59, batch size  8400, episode return 1.6e+07, actual_val_score_ind 1.3e+07\n",
      "Episode  43, epsilon   0.58, batch size  8600, episode return 3.2e+07, actual_val_score_ind 1.3e+07\n",
      "Episode  44, epsilon   0.57, batch size  8800, episode return 1.9e+07, actual_val_score_ind 1.4e+07\n",
      "Episode  45, epsilon   0.56, batch size  9000, episode return 3.7e+07, actual_val_score_ind 1.3e+07\n",
      "Episode  46, epsilon   0.55, batch size  9200, episode return 3.2e+07, actual_val_score_ind 1.3e+07\n",
      "Episode  47, epsilon   0.54, batch size  9400, episode return 2.7e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  48, epsilon   0.53, batch size  9600, episode return 2.5e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  49, epsilon   0.52, batch size  9800, episode return 5.3e+07, actual_val_score_ind 1.3e+07\n",
      "Episode  50, epsilon   0.51, batch size 10000, episode return 4.5e+07, actual_val_score_ind 1.1e+07\n",
      "Episode  51, epsilon   0.51, batch size 10200, episode return 1.8e+07, actual_val_score_ind 1.4e+07\n",
      "Episode  52, epsilon   0.50, batch size 10400, episode return 4.4e+07, actual_val_score_ind 1.4e+07\n",
      "Episode  53, epsilon   0.49, batch size 10600, episode return 5.5e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  54, epsilon   0.48, batch size 10800, episode return 2.5e+07, actual_val_score_ind 1.3e+07\n",
      "Episode  55, epsilon   0.47, batch size 11000, episode return 4.8e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  56, epsilon   0.46, batch size 11200, episode return 2.4e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  57, epsilon   0.45, batch size 11400, episode return 3.2e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  58, epsilon   0.44, batch size 11600, episode return 3.4e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  59, epsilon   0.43, batch size 11800, episode return 4.1e+07, actual_val_score_ind 1.0e+07\n",
      "Episode  60, epsilon   0.42, batch size 12000, episode return 4.5e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  61, epsilon   0.41, batch size 12200, episode return 4.0e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  62, epsilon   0.40, batch size 12400, episode return 3.4e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  63, epsilon   0.39, batch size 12600, episode return 7.4e+07, actual_val_score_ind 1.2e+07\n",
      "Episode  64, epsilon   0.38, batch size 12800, episode return 3.3e+07, actual_val_score_ind 1.9e+07\n",
      "Episode  65, epsilon   0.37, batch size 13000, episode return 5.3e+07, actual_val_score_ind 1.1e+07\n",
      "Episode  66, epsilon   0.36, batch size 13200, episode return 5.5e+07, actual_val_score_ind 1.9e+07\n",
      "Episode  67, epsilon   0.35, batch size 13400, episode return 3.5e+07, actual_val_score_ind 1.0e+07\n",
      "Episode  68, epsilon   0.34, batch size 13600, episode return 7.8e+07, actual_val_score_ind 1.9e+07\n",
      "Episode  69, epsilon   0.33, batch size 13800, episode return 6.3e+07, actual_val_score_ind 1.5e+07\n",
      "Episode  70, epsilon   0.32, batch size 14000, episode return 7.2e+07, actual_val_score_ind 1.9e+07\n",
      "Episode  71, epsilon   0.31, batch size 14200, episode return 1.8e+08, actual_val_score_ind 1.4e+07\n",
      "Episode  72, epsilon   0.30, batch size 14400, episode return 2.2e+08, actual_val_score_ind 1.9e+07\n",
      "Episode  73, epsilon   0.29, batch size 14600, episode return 1.0e+08, actual_val_score_ind 1.0e+07\n",
      "Episode  74, epsilon   0.28, batch size 14800, episode return 1.4e+08, actual_val_score_ind 1.9e+07\n",
      "Episode  75, epsilon   0.27, batch size 15000, episode return 1.3e+08, actual_val_score_ind 1.5e+07\n",
      "Episode  76, epsilon   0.26, batch size 15200, episode return 3.4e+08, actual_val_score_ind 1.4e+07\n",
      "Episode  77, epsilon   0.25, batch size 15400, episode return 1.0e+08, actual_val_score_ind 1.9e+07\n",
      "Episode  78, epsilon   0.24, batch size 15600, episode return 1.6e+08, actual_val_score_ind 1.9e+07\n",
      "Episode  79, epsilon   0.23, batch size 15800, episode return 1.7e+08, actual_val_score_ind 1.2e+07\n",
      "Episode  80, epsilon   0.22, batch size 16000, episode return 2.0e+08, actual_val_score_ind 1.4e+07\n",
      "Episode  81, epsilon   0.21, batch size 16200, episode return 2.8e+08, actual_val_score_ind 1.7e+07\n",
      "Episode  82, epsilon   0.20, batch size 16400, episode return 2.5e+08, actual_val_score_ind 1.4e+07\n",
      "Episode  83, epsilon   0.19, batch size 16600, episode return 2.4e+08, actual_val_score_ind 1.8e+07\n",
      "Episode  84, epsilon   0.18, batch size 16800, episode return 4.8e+08, actual_val_score_ind 1.5e+07\n",
      "Episode  85, epsilon   0.17, batch size 17000, episode return 3.7e+08, actual_val_score_ind 2.0e+07\n",
      "Episode  86, epsilon   0.16, batch size 17200, episode return 7.1e+07, actual_val_score_ind 1.9e+07\n",
      "Episode  87, epsilon   0.15, batch size 17400, episode return 4.5e+08, actual_val_score_ind 1.9e+07\n",
      "Episode  88, epsilon   0.14, batch size 17600, episode return 1.1e+09, actual_val_score_ind 3.1e+07\n",
      "better model\n",
      "Episode  89, epsilon   0.13, batch size 17800, episode return 2.7e+08, actual_val_score_ind 2.7e+07\n",
      "Episode  90, epsilon   0.12, batch size 18000, episode return 3.7e+08, actual_val_score_ind 1.4e+07\n",
      "Episode  91, epsilon   0.11, batch size 18200, episode return 1.4e+09, actual_val_score_ind 2.7e+07\n",
      "Episode  92, epsilon   0.10, batch size 18400, episode return 4.5e+09, actual_val_score_ind 2.7e+07\n",
      "Episode  93, epsilon   0.09, batch size 18600, episode return 1.3e+08, actual_val_score_ind 2.7e+07\n",
      "Episode  94, epsilon   0.08, batch size 18800, episode return 4.6e+08, actual_val_score_ind 1.4e+07\n",
      "Episode  95, epsilon   0.07, batch size 19000, episode return 4.3e+09, actual_val_score_ind 1.2e+07\n",
      "Episode  96, epsilon   0.06, batch size 19200, episode return 3.0e+09, actual_val_score_ind 2.5e+07\n",
      "Episode  97, epsilon   0.05, batch size 19400, episode return 4.1e+09, actual_val_score_ind 1.1e+07\n",
      "Episode  98, epsilon   0.04, batch size 19600, episode return 2.5e+09, actual_val_score_ind 1.3e+07\n",
      "Episode  99, epsilon   0.03, batch size 19800, episode return 4.3e+09, actual_val_score_ind 2.3e+07\n",
      "Episode 100, epsilon   0.02, batch size 20000, episode return 5.3e+09, actual_val_score_ind 1.3e+07\n",
      "Episode 101, epsilon   0.02, batch size 20200, episode return 1.5e+07, actual_val_score_ind 2.4e+07\n",
      "Episode 102, epsilon   0.02, batch size 20400, episode return 1.2e+10, actual_val_score_ind 2.8e+07\n",
      "Episode 103, epsilon   0.02, batch size 20600, episode return 2.7e+07, actual_val_score_ind 1.2e+07\n",
      "Episode 104, epsilon   0.02, batch size 20800, episode return 1.5e+10, actual_val_score_ind 1.2e+07\n",
      "Episode 105, epsilon   0.02, batch size 21000, episode return 1.4e+07, actual_val_score_ind 2.7e+07\n",
      "Episode 106, epsilon   0.02, batch size 21200, episode return 2.3e+08, actual_val_score_ind 1.9e+07\n",
      "Episode 107, epsilon   0.02, batch size 21400, episode return 1.5e+10, actual_val_score_ind 1.3e+07\n",
      "Episode 108, epsilon   0.02, batch size 21600, episode return 1.6e+10, actual_val_score_ind 1.6e+07\n",
      "Episode 109, epsilon   0.02, batch size 21800, episode return 4.2e+07, actual_val_score_ind 1.1e+07\n",
      "Episode 110, epsilon   0.02, batch size 22000, episode return 4.6e+07, actual_val_score_ind 1.2e+07\n",
      "Episode 111, epsilon   0.02, batch size 22200, episode return 4.3e+07, actual_val_score_ind 1.1e+07\n",
      "Episode 112, epsilon   0.02, batch size 22400, episode return 8.0e+09, actual_val_score_ind 1.1e+07\n",
      "Episode 113, epsilon   0.02, batch size 22600, episode return 8.0e+09, actual_val_score_ind 1.2e+07\n",
      "Episode 114, epsilon   0.02, batch size 22800, episode return 1.4e+10, actual_val_score_ind 8.6e+07\n",
      "better model\n",
      "Episode 115, epsilon   0.02, batch size 23000, episode return 1.3e+10, actual_val_score_ind 1.0e+07\n",
      "Episode 116, epsilon   0.02, batch size 23200, episode return 1.6e+10, actual_val_score_ind 9.7e+06\n",
      "Episode 117, epsilon   0.02, batch size 23400, episode return 8.2e+07, actual_val_score_ind 1.1e+07\n",
      "Episode 118, epsilon   0.02, batch size 23600, episode return 1.9e+10, actual_val_score_ind 1.2e+07\n",
      "Episode 119, epsilon   0.02, batch size 23800, episode return 1.6e+10, actual_val_score_ind 1.1e+07\n",
      "Episode 120, epsilon   0.02, batch size 24000, episode return 2.5e+10, actual_val_score_ind 1.1e+07\n",
      "Episode 121, epsilon   0.02, batch size 24200, episode return 1.7e+09, actual_val_score_ind 1.2e+07\n",
      "Episode 122, epsilon   0.02, batch size 24400, episode return 1.3e+10, actual_val_score_ind 1.0e+07\n",
      "Episode 123, epsilon   0.02, batch size 24600, episode return 1.7e+10, actual_val_score_ind 2.0e+07\n",
      "Episode 124, epsilon   0.02, batch size 24800, episode return 1.7e+10, actual_val_score_ind 1.8e+07\n",
      "Episode 125, epsilon   0.02, batch size 25000, episode return 2.0e+10, actual_val_score_ind 4.9e+07\n",
      "Episode 126, epsilon   0.02, batch size 25200, episode return 2.7e+08, actual_val_score_ind 5.9e+07\n",
      "Episode 127, epsilon   0.02, batch size 25400, episode return 2.1e+10, actual_val_score_ind 6.4e+07\n",
      "Episode 128, epsilon   0.02, batch size 25600, episode return 1.2e+10, actual_val_score_ind 1.8e+07\n",
      "Episode 129, epsilon   0.02, batch size 25800, episode return 1.5e+08, actual_val_score_ind 1.4e+09\n",
      "better model\n",
      "Episode 130, epsilon   0.02, batch size 26000, episode return 2.5e+10, actual_val_score_ind 4.9e+07\n",
      "Episode 131, epsilon   0.02, batch size 26200, episode return 1.8e+09, actual_val_score_ind 9.7e+08\n",
      "Episode 132, epsilon   0.02, batch size 26400, episode return 5.3e+08, actual_val_score_ind 1.9e+09\n",
      "better model\n",
      "Episode 133, epsilon   0.02, batch size 26600, episode return 1.1e+10, actual_val_score_ind 6.2e+07\n",
      "Episode 134, epsilon   0.02, batch size 26800, episode return 2.8e+10, actual_val_score_ind 6.1e+07\n",
      "Episode 135, epsilon   0.02, batch size 27000, episode return 1.5e+10, actual_val_score_ind 2.6e+09\n",
      "better model\n",
      "Episode 136, epsilon   0.02, batch size 27200, episode return 3.6e+09, actual_val_score_ind 1.7e+09\n",
      "Episode 137, epsilon   0.02, batch size 27400, episode return 2.2e+10, actual_val_score_ind 2.2e+09\n",
      "Episode 138, epsilon   0.02, batch size 27600, episode return 8.2e+09, actual_val_score_ind 1.9e+09\n",
      "Episode 139, epsilon   0.02, batch size 27800, episode return 9.9e+08, actual_val_score_ind 2.2e+09\n",
      "Episode 140, epsilon   0.02, batch size 28000, episode return 1.4e+10, actual_val_score_ind 1.2e+10\n",
      "better model\n",
      "Episode 141, epsilon   0.02, batch size 28200, episode return 1.4e+10, actual_val_score_ind 1.0e+10\n",
      "Episode 142, epsilon   0.02, batch size 28400, episode return 2.9e+10, actual_val_score_ind 1.0e+08\n",
      "Episode 143, epsilon   0.02, batch size 28600, episode return 1.7e+10, actual_val_score_ind 6.6e+07\n",
      "Episode 144, epsilon   0.02, batch size 28800, episode return 3.1e+10, actual_val_score_ind 6.7e+09\n",
      "Episode 145, epsilon   0.02, batch size 29000, episode return 1.9e+10, actual_val_score_ind 9.4e+09\n",
      "Episode 146, epsilon   0.02, batch size 29200, episode return 7.8e+09, actual_val_score_ind 2.3e+09\n",
      "Episode 147, epsilon   0.02, batch size 29400, episode return 2.9e+10, actual_val_score_ind 1.4e+10\n",
      "better model\n",
      "Episode 148, epsilon   0.02, batch size 29600, episode return 2.4e+10, actual_val_score_ind 1.3e+10\n",
      "Episode 149, epsilon   0.02, batch size 29800, episode return 3.1e+10, actual_val_score_ind 7.1e+07\n",
      "Episode 150, epsilon   0.02, batch size 30000, episode return 2.7e+10, actual_val_score_ind 8.6e+09\n",
      "Episode 151, epsilon   0.02, batch size 30200, episode return 2.3e+10, actual_val_score_ind 1.4e+10\n",
      "better model\n",
      "Episode 152, epsilon   0.02, batch size 30400, episode return 2.2e+10, actual_val_score_ind 9.1e+08\n",
      "Episode 153, epsilon   0.02, batch size 30600, episode return 1.8e+10, actual_val_score_ind 2.8e+09\n",
      "Episode 154, epsilon   0.02, batch size 30800, episode return 2.4e+10, actual_val_score_ind 9.9e+09\n",
      "Episode 155, epsilon   0.02, batch size 31000, episode return 2.4e+10, actual_val_score_ind 2.0e+09\n",
      "Episode 156, epsilon   0.02, batch size 31200, episode return 2.3e+10, actual_val_score_ind 2.7e+09\n",
      "Episode 157, epsilon   0.02, batch size 31400, episode return 3.0e+10, actual_val_score_ind 1.4e+10\n",
      "better model\n",
      "Episode 158, epsilon   0.02, batch size 31600, episode return 1.8e+10, actual_val_score_ind 1.4e+10\n",
      "Episode 159, epsilon   0.02, batch size 31800, episode return 2.5e+10, actual_val_score_ind 1.0e+10\n",
      "Episode 160, epsilon   0.02, batch size 32000, episode return 1.8e+10, actual_val_score_ind 1.9e+09\n",
      "Episode 161, epsilon   0.02, batch size 32200, episode return 6.1e+08, actual_val_score_ind 7.0e+09\n",
      "Episode 162, epsilon   0.02, batch size 32400, episode return 1.9e+10, actual_val_score_ind 1.3e+10\n",
      "Episode 163, epsilon   0.02, batch size 32600, episode return 1.3e+10, actual_val_score_ind 9.8e+09\n",
      "Episode 164, epsilon   0.02, batch size 32800, episode return 2.2e+10, actual_val_score_ind 2.6e+09\n",
      "Episode 165, epsilon   0.02, batch size 33000, episode return 1.1e+10, actual_val_score_ind 1.1e+10\n",
      "Episode 166, epsilon   0.02, batch size 33200, episode return 2.4e+10, actual_val_score_ind 1.6e+10\n",
      "better model\n",
      "Episode 167, epsilon   0.02, batch size 33400, episode return 2.3e+09, actual_val_score_ind 1.7e+10\n",
      "better model\n",
      "Episode 168, epsilon   0.02, batch size 33600, episode return 2.4e+10, actual_val_score_ind 2.2e+10\n",
      "better model\n",
      "Episode 169, epsilon   0.02, batch size 33800, episode return 2.1e+10, actual_val_score_ind 1.0e+10\n",
      "Episode 170, epsilon   0.02, batch size 34000, episode return 8.0e+09, actual_val_score_ind 1.2e+10\n",
      "Episode 171, epsilon   0.02, batch size 34200, episode return 2.6e+10, actual_val_score_ind 3.6e+09\n",
      "Episode 172, epsilon   0.02, batch size 34400, episode return 4.5e+08, actual_val_score_ind 1.4e+10\n",
      "Episode 173, epsilon   0.02, batch size 34600, episode return 2.5e+10, actual_val_score_ind 1.4e+10\n",
      "Episode 174, epsilon   0.02, batch size 34800, episode return 1.2e+10, actual_val_score_ind 7.3e+09\n",
      "Episode 175, epsilon   0.02, batch size 35000, episode return 2.4e+10, actual_val_score_ind 1.5e+10\n",
      "Episode 176, epsilon   0.02, batch size 35200, episode return 1.8e+10, actual_val_score_ind 1.6e+10\n",
      "Episode 177, epsilon   0.02, batch size 35400, episode return 3.4e+09, actual_val_score_ind 4.9e+09\n",
      "Episode 178, epsilon   0.02, batch size 35600, episode return 1.3e+10, actual_val_score_ind 1.6e+10\n",
      "Episode 179, epsilon   0.02, batch size 35800, episode return 1.9e+10, actual_val_score_ind 8.3e+09\n",
      "Episode 180, epsilon   0.02, batch size 36000, episode return 3.3e+10, actual_val_score_ind 1.7e+10\n",
      "Episode 181, epsilon   0.02, batch size 36200, episode return 8.0e+09, actual_val_score_ind 5.5e+09\n",
      "Episode 182, epsilon   0.02, batch size 36400, episode return 1.2e+10, actual_val_score_ind 1.6e+10\n",
      "Episode 183, epsilon   0.02, batch size 36600, episode return 2.4e+10, actual_val_score_ind 1.5e+10\n",
      "Episode 184, epsilon   0.02, batch size 36800, episode return 2.3e+10, actual_val_score_ind 9.5e+09\n",
      "Episode 185, epsilon   0.02, batch size 37000, episode return 1.1e+10, actual_val_score_ind 1.8e+09\n",
      "Episode 186, epsilon   0.02, batch size 37200, episode return 2.3e+10, actual_val_score_ind 1.3e+10\n",
      "Episode 187, epsilon   0.02, batch size 37400, episode return 1.7e+10, actual_val_score_ind 2.1e+09\n",
      "Episode 188, epsilon   0.02, batch size 37600, episode return 1.5e+10, actual_val_score_ind 6.3e+09\n",
      "Episode 189, epsilon   0.02, batch size 37800, episode return 2.9e+10, actual_val_score_ind 3.0e+08\n",
      "Episode 190, epsilon   0.02, batch size 38000, episode return 1.8e+10, actual_val_score_ind 2.9e+08\n",
      "Episode 191, epsilon   0.02, batch size 38200, episode return 5.2e+08, actual_val_score_ind 4.8e+08\n",
      "Episode 192, epsilon   0.02, batch size 38400, episode return 3.5e+08, actual_val_score_ind 4.6e+08\n",
      "Episode 193, epsilon   0.02, batch size 38600, episode return 3.3e+09, actual_val_score_ind 1.2e+10\n",
      "Episode 194, epsilon   0.02, batch size 38800, episode return 1.4e+10, actual_val_score_ind 1.5e+10\n",
      "Episode 195, epsilon   0.02, batch size 39000, episode return 2.2e+10, actual_val_score_ind 9.2e+09\n",
      "Episode 196, epsilon   0.02, batch size 39200, episode return 1.1e+10, actual_val_score_ind 1.6e+09\n",
      "Episode 197, epsilon   0.02, batch size 39400, episode return 8.3e+08, actual_val_score_ind 1.5e+10\n",
      "Episode 198, epsilon   0.02, batch size 39600, episode return 2.3e+10, actual_val_score_ind 2.7e+08\n",
      "Episode 199, epsilon   0.02, batch size 39800, episode return 1.0e+10, actual_val_score_ind 1.4e+10\n",
      "Episode 200, epsilon   0.02, batch size 40000, episode return 8.3e+08, actual_val_score_ind 3.5e+09\n",
      "Episode 201, epsilon   0.02, batch size 40200, episode return 2.2e+10, actual_val_score_ind 2.5e+10\n",
      "better model\n",
      "Episode 202, epsilon   0.02, batch size 40400, episode return 4.1e+08, actual_val_score_ind 2.1e+08\n",
      "Episode 203, epsilon   0.02, batch size 40600, episode return 1.8e+10, actual_val_score_ind 2.4e+08\n",
      "Episode 204, epsilon   0.02, batch size 40800, episode return 9.0e+09, actual_val_score_ind 2.5e+08\n",
      "Episode 205, epsilon   0.02, batch size 41000, episode return 2.0e+10, actual_val_score_ind 2.1e+08\n",
      "Episode 206, epsilon   0.02, batch size 41200, episode return 9.2e+08, actual_val_score_ind 1.3e+08\n",
      "Episode 207, epsilon   0.02, batch size 41400, episode return 8.0e+08, actual_val_score_ind 1.0e+08\n",
      "Episode 208, epsilon   0.02, batch size 41600, episode return 3.6e+08, actual_val_score_ind 1.1e+08\n",
      "Episode 209, epsilon   0.02, batch size 41800, episode return 7.5e+08, actual_val_score_ind 9.7e+07\n",
      "Episode 210, epsilon   0.02, batch size 42000, episode return 1.4e+09, actual_val_score_ind 1.6e+08\n",
      "Episode 211, epsilon   0.02, batch size 42200, episode return 3.1e+08, actual_val_score_ind 3.2e+08\n",
      "Episode 212, epsilon   0.02, batch size 42400, episode return 4.0e+08, actual_val_score_ind 1.5e+08\n",
      "Episode 213, epsilon   0.02, batch size 42600, episode return 1.1e+08, actual_val_score_ind 1.1e+08\n",
      "Episode 214, epsilon   0.02, batch size 42800, episode return 2.5e+08, actual_val_score_ind 1.9e+08\n",
      "Episode 215, epsilon   0.02, batch size 43000, episode return 1.4e+10, actual_val_score_ind 1.1e+08\n",
      "Episode 216, epsilon   0.02, batch size 43200, episode return 7.7e+08, actual_val_score_ind 9.5e+07\n",
      "Episode 217, epsilon   0.02, batch size 43400, episode return 8.7e+07, actual_val_score_ind 1.3e+08\n",
      "Episode 218, epsilon   0.02, batch size 43600, episode return 2.1e+08, actual_val_score_ind 8.0e+07\n",
      "Episode 219, epsilon   0.02, batch size 43800, episode return 1.1e+08, actual_val_score_ind 1.2e+08\n",
      "Episode 220, epsilon   0.02, batch size 44000, episode return 3.5e+08, actual_val_score_ind 9.7e+07\n",
      "Episode 221, epsilon   0.02, batch size 44200, episode return 1.0e+09, actual_val_score_ind 1.2e+08\n",
      "Episode 222, epsilon   0.02, batch size 44400, episode return 1.3e+09, actual_val_score_ind 1.2e+08\n",
      "Episode 223, epsilon   0.02, batch size 44600, episode return 9.9e+07, actual_val_score_ind 7.8e+07\n",
      "Episode 224, epsilon   0.02, batch size 44800, episode return 8.5e+08, actual_val_score_ind 6.8e+07\n",
      "Episode 225, epsilon   0.02, batch size 45000, episode return 1.3e+09, actual_val_score_ind 9.0e+07\n",
      "Episode 226, epsilon   0.02, batch size 45200, episode return 1.3e+09, actual_val_score_ind 6.4e+07\n",
      "Episode 227, epsilon   0.02, batch size 45400, episode return 6.5e+07, actual_val_score_ind 6.7e+07\n",
      "Episode 228, epsilon   0.02, batch size 45600, episode return 1.6e+08, actual_val_score_ind 7.3e+07\n",
      "Episode 229, epsilon   0.02, batch size 45800, episode return 1.2e+08, actual_val_score_ind 1.2e+08\n",
      "Episode 230, epsilon   0.02, batch size 46000, episode return 1.0e+08, actual_val_score_ind 1.0e+08\n",
      "Episode 231, epsilon   0.02, batch size 46200, episode return 9.9e+08, actual_val_score_ind 6.8e+07\n",
      "Episode 232, epsilon   0.02, batch size 46400, episode return 7.3e+07, actual_val_score_ind 8.5e+07\n",
      "Episode 233, epsilon   0.02, batch size 46600, episode return 1.1e+09, actual_val_score_ind 7.6e+07\n",
      "Episode 234, epsilon   0.02, batch size 46800, episode return 8.6e+07, actual_val_score_ind 6.6e+07\n",
      "Episode 235, epsilon   0.02, batch size 47000, episode return 6.5e+08, actual_val_score_ind 7.8e+07\n",
      "Episode 236, epsilon   0.02, batch size 47200, episode return 2.7e+09, actual_val_score_ind 5.9e+07\n",
      "Episode 237, epsilon   0.02, batch size 47400, episode return 8.4e+07, actual_val_score_ind 6.1e+07\n",
      "Episode 238, epsilon   0.02, batch size 47600, episode return 8.4e+07, actual_val_score_ind 6.7e+07\n",
      "Episode 239, epsilon   0.02, batch size 47800, episode return 6.2e+07, actual_val_score_ind 4.6e+07\n",
      "Episode 240, epsilon   0.02, batch size 48000, episode return 4.6e+09, actual_val_score_ind 6.4e+07\n",
      "Episode 241, epsilon   0.02, batch size 48200, episode return 2.0e+07, actual_val_score_ind 4.3e+07\n",
      "Episode 242, epsilon   0.02, batch size 48400, episode return 2.3e+07, actual_val_score_ind 4.6e+07\n",
      "Episode 243, epsilon   0.02, batch size 48600, episode return 7.0e+07, actual_val_score_ind 4.8e+07\n",
      "Episode 244, epsilon   0.02, batch size 48800, episode return 1.3e+08, actual_val_score_ind 6.6e+07\n",
      "Episode 245, epsilon   0.02, batch size 49000, episode return 2.0e+07, actual_val_score_ind 5.5e+07\n",
      "Episode 246, epsilon   0.02, batch size 49200, episode return 3.1e+07, actual_val_score_ind 6.1e+07\n",
      "Episode 247, epsilon   0.02, batch size 49400, episode return 1.1e+10, actual_val_score_ind 5.9e+07\n",
      "Episode 248, epsilon   0.02, batch size 49600, episode return 2.7e+07, actual_val_score_ind 5.9e+07\n",
      "Episode 249, epsilon   0.02, batch size 49800, episode return 1.8e+07, actual_val_score_ind 4.2e+07\n",
      "Episode 250, epsilon   0.02, batch size 50000, episode return 3.0e+07, actual_val_score_ind 1.4e+07\n",
      "Episode 251, epsilon   0.02, batch size 50200, episode return 1.1e+10, actual_val_score_ind 3.5e+07\n",
      "Episode 252, epsilon   0.02, batch size 50400, episode return 2.0e+07, actual_val_score_ind 3.4e+06\n",
      "Episode 253, epsilon   0.02, batch size 50600, episode return 1.5e+07, actual_val_score_ind 3.6e+07\n",
      "Episode 254, epsilon   0.02, batch size 50800, episode return 1.8e+07, actual_val_score_ind 2.3e+07\n",
      "Episode 255, epsilon   0.02, batch size 51000, episode return 2.3e+07, actual_val_score_ind 4.8e+07\n",
      "Episode 256, epsilon   0.02, batch size 51200, episode return 4.0e+07, actual_val_score_ind 4.3e+07\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ProjectAgent' object has no attribute 'network'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m agent \u001b[38;5;241m=\u001b[39m ProjectAgent()\n\u001b[1;32m     23\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 24\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Keep the following lines to evaluate your agent unchanged.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m score_agent: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m evaluate_HIV(agent\u001b[38;5;241m=\u001b[39magent, nb_episode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/rl-class-assignment-damienv66/src/train_dams.py:55\u001b[0m, in \u001b[0;36mProjectAgent.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/model5.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m({}, device)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ProjectAgent' object has no attribute 'network'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population\n",
    "from train_dams import ProjectAgent  # Replace DummyAgent with your agent implementation\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(seed=42)\n",
    "# Initialization of the agent. Replace DummyAgent with your custom agent implementation.\n",
    "agent = ProjectAgent()\n",
    "agent.train()\n",
    "agent.load()\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population\n",
    "from train_dams import ProjectAgent  # Replace DummyAgent with your agent implementation\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_everything(seed=42)\n",
    "# Initialization of the agent. Replace DummyAgent with your custom agent implementation.\n",
    "agent = ProjectAgent()\n",
    "#agent.train()\n",
    "agent.load()\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score_model5.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
